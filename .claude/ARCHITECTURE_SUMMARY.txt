================================================================================
DiffSBDD CODEBASE ARCHITECTURE - QUICK REFERENCE
================================================================================

WHAT IS DiffSBDD?
================================================================================
An equivariant diffusion model that generates novel drug-like molecules to bind
protein pockets. Given a 3D protein binding site, it samples new ligands that:
  - Have valid 3D geometries (bond distances, angles)
  - Are chemically valid (proper valencies)
  - Potentially bind favorably to the target

KEY INSIGHT: The model preserves SE(3) equivariance - it respects rotations and
translations, which is crucial for 3D molecular generation.


MAIN COMPONENTS
================================================================================

1. TRAINING ENGINE: LigandPocketDDPM (lightning_modules.py, 1079 lines)
   ├─ PyTorch Lightning module for orchestrating training
   ├─ Loads data via ProcessedLigandPocketDataset (dataset.py)
   ├─ Integrates diffusion model (en_diffusion.py or conditional_model.py)
   ├─ Integrates denoising network (EGNNDynamics in dynamics.py)
   ├─ Computes loss and metrics during training/evaluation
   └─ Provides generate_ligands() method for inference

2. DIFFUSION MODEL: EnVariationalDiffusion (en_diffusion.py, 1190 lines)
   ├─ Implements forward process (adding noise: x → z_t)
   ├─ Implements reverse process (removing noise: z_t → x via network)
   ├─ Manages noise schedule (learned or predefined)
   ├─ Computes variational lower bound loss
   ├─ Two variants:
   │  ├─ EnVariationalDiffusion: Joint mode (diffuse pocket + ligand)
   │  └─ ConditionalDDPM: Conditional (pocket fixed, sample ligand only)
   └─ DistributionNodes: Models P(ligand_size | pocket_size)

3. DENOISING NETWORK: EGNNDynamics (dynamics.py, 278 lines)
   ├─ Predicts noise to remove at each diffusion step
   ├─ Based on Equivariant Graph Neural Network (EGNN)
   ├─ Input: [x_atoms, h_atoms, x_residues, h_residues, t]
   ├─ Output: [eps_x_atoms, eps_h_atoms, eps_x_residues, eps_h_residues]
   ├─ Features:
   │  ├─ Encodes atoms/residues to shared feature space
   │  ├─ Passes through EGNN with multiple layers
   │  ├─ Decodes back to original feature dimensions
   │  ├─ Supports edge cutoffs (distance-based interactions)
   │  └─ Can use edge embeddings for bond type information
   └─ Currently has placeholder for ESM-C protein embeddings

4. EGNN NETWORK: EGNN (egnn_new.py, 511 lines)
   ├─ Equivariant Graph Neural Network for geometry-aware updates
   ├─ Key design:
   │  ├─ Node features h are invariant (don't change under rotations)
   │  ├─ Coordinates x are equivariant (rotate/translate with input)
   │  ├─ Edge vectors r_ij = x_i - x_j used in invariant messages
   │  ├─ Coordinate updates are direction-preserving
   │  └─ Multiple layers with attention, normalization options
   └─ Respects SE(3) symmetry: essential for 3D generation

5. DATA PROCESSING
   ├─ ProcessedLigandPocketDataset (dataset.py, 71 lines)
   │  ├─ Loads pre-processed NPZ files
   │  ├─ Each file contains: ligand & pocket coords, atom types, masks
   │  ├─ Splits by mask and applies centering
   │  ├─ Custom collate_fn handles batching with scatter-friendly masks
   │  └─ Supports transforms (e.g., AppendVirtualNodes)
   │
   ├─ process_crossdock.py (18348 bytes) & process_bindingmoad.py (25281 bytes)
   │  ├─ Raw data preparation from PDB files
   │  ├─ Pocket extraction (by distance cutoff from ligand)
   │  ├─ Creates train/val/test splits
   │  └─ Outputs NPZ files with normalized tensors
   │
   └─ constants.py (463 lines)
      ├─ Bond length tables for single/double/triple bonds
      ├─ Atom valency rules
      ├─ Dataset-specific parameters (atom encodings, distributions)
      └─ LJ parameters for clash penalty

6. POST-PROCESSING & ANALYSIS
   ├─ molecule_builder.py (269 lines)
   │  ├─ get_bond_order(): distance → bond type via lookup tables
   │  ├─ make_mol_openbabel(): robust bond inference using OpenBabel
   │  ├─ make_mol_edm(): fast bond inference from distance tables
   │  ├─ build_molecule(): main entry point (uses OpenBabel by default)
   │  ├─ process_molecule(): sanitization, H addition, UFF relaxation
   │  └─ filter_rd_mol(): removes invalid ring structures
   │
   ├─ metrics.py (245 lines)
   │  ├─ CategoricalDistribution: tracks atom type distributions, KL divergence
   │  ├─ BasicMolecularMetrics:
   │  │  ├─ compute_validity(): checks chemical validity
   │  │  ├─ compute_connectivity(): checks if molecules are connected
   │  │  ├─ compute_uniqueness(): deduplicates by SMILES
   │  │  └─ compute_novelty(): checks against training set
   │  └─ MoleculeProperties: QED, SA Score, LogP, Lipinski rules
   │
   ├─ visualization.py (452 lines)
   │  ├─ save_xyz_file(): XYZ format output
   │  ├─ visualize(): PNG rendering with atom colors
   │  └─ visualize_chain(): animation of diffusion trajectory
   │
   ├─ docking.py (247 lines)
   │  └─ smina_score(): molecular docking scoring
   │
   └─ utils.py (235 lines)
      ├─ Queue: fixed-size queue for gradient norm tracking
      ├─ residues_to_atoms(): CA → all-atom representation
      ├─ get_pocket_from_ligand(): pocket extraction by distance
      ├─ batch_to_list(): split batch tensor by mask indices
      ├─ num_nodes_to_batch_mask(): create mask from node counts
      ├─ calc_rmsd(): RMSD with unknown atom correspondence
      └─ AppendVirtualNodes: pad ligands with dummy atoms for fixed size


DATA FLOW
================================================================================

TRAINING:
  Raw PDB files
    ↓
  process_crossdock.py or process_bindingmoad.py
    ↓
  NPZ files (train.npz, val.npz, test.npz)
    ↓
  ProcessedLigandPocketDataset
    ↓
  DataLoader (collate_fn creates batch tensors with masks)
    ↓
  LigandPocketDDPM.forward()
    ├─ Extract ligand & pocket dicts
    ├─ Sample timestep t
    ├─ Add noise to get z_t
    ├─ EGNNDynamics predicts noise
    ├─ Compute MSE loss (or VLB)
    └─ Backprop via AdamW optimizer
    ↓
  WandB logs metrics & saves checkpoints


INFERENCE:
  Checkpoint file (.ckpt)
    ↓
  LigandPocketDDPM.load_from_checkpoint()
    ↓
  model.generate_ligands(pdb_file, pocket_def, n_samples)
    ├─ Parse PDB, extract pocket residues
    ├─ Sample ligand size from distribution
    ├─ Initialize random z_T
    ├─ Loop t=T to 1: denoise one step (call EGNNDynamics)
    ├─ Extract final x and h (coordinates and atom types)
    ↓
  build_molecule() → RDKit Mol objects
    ├─ Infer bonds from coordinates via OpenBabel
    ├─ Add conformer (3D positions)
    └─ Returns RDKit molecule
    ↓
  process_molecule() → filtered molecules
    ├─ Sanitize (fix valencies)
    ├─ Keep largest fragment
    ├─ UFF force field optimization
    └─ Returns valid molecules
    ↓
  write_sdf_file() → output.sdf


KEY TRAINING CONCEPTS
================================================================================

LOSS COMPUTATION:
  Forward diffusion: x_0 --noise--> z_t = α_t*x_0 + σ_t*ε
    where α_t = sqrt(1-σ_t²), monotonic schedule
  
  Training objective (L2):
    L = ||ε_pred - ε_true||² / denominator
    (normalized by number of atoms and feature dimensions)
  
  Alternative (VLB - variational lower bound):
    L = -T * SNR_weight * ||ε_pred - ε_true||²
    (more principled but slower)

NOISE SCHEDULE:
  Controls how much noise at each timestep
  Options: learned, polynomial_2, cosine, linear, sqrt
  Key: γ_t = log(α_t²) - log(σ_t²), monotonic γ_0 > γ_T

VIRTUAL NODES (optional):
  Pads ligands with dummy "Ne" atoms to max size
  Advantages: fixed tensor shapes, better batching
  Disadvantages: dummy atoms need masking in loss

GRADIENT CLIPPING (optional):
  max_grad_norm = 1.5 * mean(recent_norms) + 2 * std(recent_norms)
  Prevents exploding gradients

AUXILIARY LOSS (optional):
  Lennard-Jones potential penalty for atom clashes
  Weighted schedule: strong in early diffusion, weak at t=0


CONFIGURATION SYSTEM
================================================================================

YAML Config Files (configs/*.yml)
├─ run_name: experiment identifier
├─ logdir, datadir: paths
├─ wandb_params: logging settings
├─ mode: "joint" or "pocket_conditioning"
├─ pocket_representation: "CA" or "full-atom"
├─ virtual_nodes: boolean
├─ batch_size, lr, n_epochs, gpus: training basics
├─ clip_grad: gradient clipping on/off
├─ egnn_params:
│  ├─ joint_nf: shared feature dimension (e.g., 128)
│  ├─ hidden_nf: EGNN hidden layers (e.g., 256)
│  ├─ n_layers: EGNN depth (e.g., 6)
│  ├─ attention: True/False
│  ├─ edge_cutoff_*: distance limits (e.g., 5.0 Å)
│  └─ Other: tanh, norm_constant, aggregation_method
├─ diffusion_params:
│  ├─ diffusion_steps: T (e.g., 500)
│  ├─ diffusion_noise_schedule: e.g., "polynomial_2"
│  ├─ diffusion_loss_type: "l2" or "vlb"
│  └─ normalize_factors: [norm_x, norm_h] e.g., [1, 4]
└─ eval_params:
   ├─ n_eval_samples: metrics sample count
   ├─ eval_batch_size: evaluation batch size
   └─ smiles_file: optional training SMILES for novelty metric


QUICK START COMMANDS
================================================================================

1. TRAIN:
   python train.py --config configs/crossdock_fullatom_cond.yml

2. GENERATE LIGANDS:
   python generate_ligands.py checkpoints/model.ckpt \
       --pdbfile example.pdb \
       --outfile output.sdf \
       --ref_ligand A:330 \  # or --resi_list A:1 A:2 ...
       --n_samples 20

3. INPAINT (scaffold elaboration):
   python inpaint.py checkpoints/model.ckpt \
       --pdbfile example.pdb \
       --outfile output.sdf \
       --ref_ligand example_lig.sdf \
       --fix_atoms fragment.sdf \
       --add_n_nodes 10

4. OPTIMIZE (evolutionary):
   python optimize.py \
       --checkpoint checkpoints/model.ckpt \
       --pdbfile example.pdb \
       --outfile output.sdf \
       --objective sa \
       --population_size 100 \
       --evolution_steps 10


DEBUGGING CHECKLIST
================================================================================

□ Check gradient norms in WandB (should be stable)
□ Monitor loss components: error_t_lig, error_t_pocket, loss_0, kl_prior
□ Check validity % in metrics (if <50%, something's wrong)
□ Look for NaN in coordinates (numerical instability)
□ Verify pocket extraction worked (print residue list)
□ Check if virtual nodes are on/off matching config
□ Ensure data normalization factors match between datasets
□ If bonds are wrong, check molecule_builder.py bond tables
□ If molecules crash into pocket, increase auxiliary_loss weight
□ If sampling is slow, reduce timesteps or batch_size


IMPORTANT IMPLEMENTATION NOTES
================================================================================

1. MASKS: Batch indices for torch_scatter operations
   Example: mask=[0,0,0,0,0,1,1,1,2,2,2,2] means:
   - Atoms 0-4 belong to sample 0
   - Atoms 5-7 belong to sample 1
   - Atoms 8-11 belong to sample 2

2. CENTER-OF-MASS CONSTRAINT:
   After each denoising step, coordinates are shifted to have
   zero mean per batch element. Essential for equivariance.

3. DIFFUSION TIMESTEPS:
   - High t: mostly noise (far from data)
   - Low t: mostly signal (close to data)
   - Forward: x_0 --add noise--> z_T
   - Reverse: z_T --remove noise--> x_0

4. ATOM TYPE PREDICTION:
   One-hot categorical logits passed through softmax
   Continuous during diffusion, discrete at inference via argmax

5. BOND ORDER DETERMINATION:
   Distance between atoms → bond type via lookup table
   OpenBabel is more robust than direct distance tables

6. COORDINATE EQUIVARIANCE:
   EGNN ensures: if input rotates, output rotates the same way
   This is critical for valid 3D generation


FILE SIZE REFERENCE
================================================================================
lightning_modules.py ............. 1079 lines (core training engine)
equivariant_diffusion/en_diffusion.py . 1190 lines (diffusion model)
equivariant_diffusion/conditional_model.py . ~500 lines (conditional variant)
equivariant_diffusion/dynamics.py .. 278 lines (denoising network)
equivariant_diffusion/egnn_new.py .. 511 lines (EGNN architecture)
dataset.py ....................... 71 lines (data loading)
analysis/metrics.py .............. 245 lines (evaluation)
analysis/molecule_builder.py ...... 269 lines (bond prediction)
analysis/visualization.py ......... 452 lines (visualization)
utils.py ......................... 235 lines (utilities)
constants.py ..................... 463 lines (bond tables, etc.)
train.py ......................... 139 lines (training entry)
generate_ligands.py .............. 61 lines (inference entry)
inpaint.py ....................... 298 lines (inpainting)
optimize.py ...................... 338 lines (optimization)
configs/*.yml .................... ~1.5KB each (8 configs)

TOTAL CORE: ~6000 lines of logic (+ tests, data, logs)


PAPER REFERENCE
================================================================================
Schneuing et al., "Structure-based drug design with equivariant diffusion
models", Nature Computational Science, 2024
doi: 10.1038/s43588-024-00737-x

Key contributions:
- SE(3)-equivariant diffusion for 3D molecules
- Joint vs conditional training modes
- Pocket representation options (CA vs full-atom)
- VLB objective for principled likelihood
- Comprehensive benchmarking on CrossDocked & Binding MOAD

================================================================================
