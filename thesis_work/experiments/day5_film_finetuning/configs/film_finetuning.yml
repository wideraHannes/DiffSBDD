# FiLM-Only Fine-Tuning Configuration
# Loads pretrained checkpoint, freezes EGNN, trains only FiLM adapter

run_name: "film-finetuning-v1"
logdir: "thesis_work/experiments/day5_film_finetuning/outputs"
wandb_params:
  mode: "online"
  entity: null
  group: "film-finetuning"

# Dataset
dataset: "crossdock_full"
datadir: "data/processed_crossdock_noH_full"
esmc_path: "data/processed_crossdock_noH_full"  # Directory with {split}_esmc.npz

# Pretrained checkpoint (EGNN will be frozen)
resume: "checkpoints/crossdocked_fullatom_cond.ckpt"
film_only_training: true  # NEW: freeze EGNN, train only FiLM

# Training - optimized for FiLM-only
batch_size: 16
lr: 1.0e-3           # Higher LR ok for small adapter
n_epochs: 50         # Much fewer epochs needed
num_workers: 4
gpus: 1
clip_grad: true

# Model architecture (must match pretrained checkpoint)
mode: "pocket_conditioning"
pocket_representation: "full-atom"
virtual_nodes: false

egnn_params:
  device: "cuda"
  edge_cutoff_ligand: null
  edge_cutoff_pocket: 5.0
  edge_cutoff_interaction: 5.0
  reflection_equivariant: false
  joint_nf: 32         # Must match checkpoint
  hidden_nf: 128       # Must match checkpoint
  n_layers: 5          # Must match checkpoint
  attention: true
  tanh: true
  norm_constant: 1
  inv_sublayers: 1
  sin_embedding: false
  aggregation_method: "sum"
  normalization_factor: 100

diffusion_params:
  diffusion_steps: 500
  diffusion_noise_schedule: "polynomial_2"
  diffusion_noise_precision: 5.0e-4
  diffusion_loss_type: "l2"
  normalize_factors: [1, 4]

# Evaluation
eval_epochs: 10
visualize_sample_epoch: 25
visualize_chain_epoch: 50
eval_params:
  n_eval_samples: 50
  eval_batch_size: 50
  smiles_file: "data/processed_crossdock_noH_full/train_smiles.npy"
  n_visualize_samples: 5
  keep_frames: 100

# Other
enable_progress_bar: true
num_sanity_val_steps: 0
augment_rotation: false
augment_noise: 0
auxiliary_loss: false
loss_params:
  max_weight: 0.001
  schedule: "linear"
  clamp_lj: 3.0
